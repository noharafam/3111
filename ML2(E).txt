[1]import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

[2]df = pd.read_csv("C:\\Users\\Ekta\\Desktop\\emails.csv")

[3]df

[4]df.head()

[5]df.isnull().sum()

[6]X = df.iloc[:,1:3001]
X

[7]Y = df.iloc[:,-1].values
Y

[8]train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size = 0.25)

[9]svc = SVC(C=1.0,kernel='rbf',gamma='auto')         
# C here is the regularization parameter. Here, L2 penalty is used(default). It is the inverse of the strength of regularization.
# As C increases, model overfits.
# Kernel here is the radial basis function kernel.
# gamma (only used for rbf kernel) : As gamma increases, model overfits.
svc.fit(train_x,train_y)
y_pred2 = svc.predict(test_x)
print("Accuracy Score for SVC : ", accuracy_score(y_pred2,test_y))

[10]X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)

[11]knn = KNeighborsClassifier(n_neighbors=7)

[12]knn.fit(X_train, y_train)

[13]print(knn.predict(X_test))

[14]print(knn.score(X_test, y_test))